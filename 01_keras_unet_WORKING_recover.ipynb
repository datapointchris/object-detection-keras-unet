{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "https://www.kaggle.com/christofhenkel/keras-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "# CURRENTLY WORKING WITH TENSORFLOW\n",
    "# \n",
    "# PRIORITIES:\n",
    "# 1. refactor code to better groupings\n",
    "# 2. where do the functions go\n",
    "# 3. grab validation data\n",
    "# 4. test testing data generator\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "# ====================================== #\n",
    "### CURRENT WORKING IMAGE INPUT PARAMS ###\n",
    "\n",
    "# IMAGES_PER_BATCH\n",
    "# original images loaded and vstacked\n",
    "# 16 images = 23 GB\n",
    "# 8 images is faster\n",
    "\n",
    "# MODEL_BATCH_SIZE\n",
    "# how many of the splitter output arrays to process at one time\n",
    "# 128 = ERROR\n",
    "# 96 is okay\n",
    "\n",
    "# SPLITS\n",
    "# row X col = total images into array\n",
    "# more splits = larger batch size\n",
    "\n",
    "### GOOD ###\n",
    "images_per_batch = 16\n",
    "split_rows = 20\n",
    "split_cols = 20\n",
    "model_batch_size = 96\n",
    "augmentation = False\n",
    "\n",
    "### NOT GOOD ###\n",
    "images_per_batch = 16\n",
    "split_rows = 20\n",
    "split_cols = 20\n",
    "model_batch_size = 96\n",
    "augmentation = True\n",
    "# with augmentation 28 GB used memory, going into swap\n",
    "\n",
    "### NOT GOOD ###\n",
    "images_per_batch = 16\n",
    "split_rows = 24\n",
    "split_cols = 24\n",
    "model_batch_size = 128\n",
    "augmentation = True\n",
    "# batch size too large to fit in GPU\n",
    "\n",
    "\n",
    "# ====================================== #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ======================================== FOR RUNNING ON THE MACBOOK PRO ====================\n",
    "\n",
    "### TODO ###\n",
    "# Check for MacOS environment to enable this automatically\n",
    "\n",
    "# ## DO THIS BEFORE IMPORTING KERAS OR TENSOR TO USE PLAIDML\n",
    "# import plaidml.keras\n",
    "# plaidml.keras.install_backend()\n",
    "\n",
    "# # Help MacOS be able to use Keras\n",
    "# import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "# # Gets rid of the processor warning.\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from callbacks import ValPlotCallback\n",
    "from model import make_model\n",
    "from keras.layers import Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from tqdm.keras import TqdmCallback\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir = 'images/training/build/0/'\n",
    "train_mask_dir = 'images/training/mask/0/'\n",
    "test_image_dir = 'images/training/test/0/'\n",
    "data_dir = 'data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 77\n",
    "train_val_split_size = 0.1\n",
    "images_per_batch = 8  # decrease this number if running out of memory\n",
    "\n",
    "# ======Image Splitter Params ====== #\n",
    "split_rows = 20\n",
    "split_cols = 20\n",
    "resize = True\n",
    "image_resize_width = 4800\n",
    "image_resize_height = 4800\n",
    "\n",
    "# ======================= MODEL PARAMS ======================= #\n",
    "epochs = 1\n",
    "model_batch_size = 64\n",
    "model_name = 'datagenmodel'\n",
    "model_path = os.path.join(data_dir, model_name + '.h5')\n",
    "pretrained_model = False\n",
    "pretrained_model_path = model_path\n",
    "print_model_summary_on_compile = False\n",
    "plot_epoch_val_images = True\n",
    "\n",
    "# ==================== Data Augmentation ==================== #\n",
    "data_augmentation = True\n",
    "datagen_args = dict(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "    # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    rotation_range=60,\n",
    "    # randomly shift images horizontally (fraction of total width)\n",
    "    width_shift_range=0.2,\n",
    "    # randomly shift images vertically (fraction of total height)\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.0,  # set range for random shear\n",
    "    zoom_range=0.0,  # set range for random zoom\n",
    "    channel_shift_range=0.0,  # set range for random channel shifts\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    cval=0.0,  # value used for fill_mode = \"constant\"\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=True,  # randomly flip images\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format='channels_last',\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0,\n",
    ")\n",
    "\n",
    "# ==================== Callbacks ==================== #\n",
    "early_stop = EarlyStopping(patience=5, verbose=1)\n",
    "check_point = ModelCheckpoint(\n",
    "    os.path.join(data_dir, model_name + '.hdf5'), save_best_only=True, verbose=1\n",
    ")\n",
    "tensor_board = TensorBoard(\n",
    "    log_dir='../logs/tensorboard/',\n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_grads=False,\n",
    "    write_images=True,\n",
    "    embeddings_freq=1,\n",
    "    update_freq='epoch',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_properties(name, array):\n",
    "    \"\"\"Prints the length shape and memory size of an array\"\"\"\n",
    "    print('==========')\n",
    "    print(name.upper())\n",
    "    print(f'Length: {len(array)}')\n",
    "    print(f'Shape: {array.shape}')\n",
    "    print(f'Size: {round(array.itemsize * array.size / 1024 / 1024 / 1024, 3)} GB')\n",
    "    print('==========')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(iterable, batch_size):\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(islice(iterator, batch_size)):\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_stacker(batches):\n",
    "    for batch in batches:\n",
    "        array = [cv2.imread(str(image)).astype(np.uint8) for image in batch]\n",
    "        stack = (np.vstack(array)/255).astype(np.float32)\n",
    "        yield stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating New Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 16:51:31.606281: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"max_pooling2d_1\" (type MaxPooling2D).\n\nNegative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_1/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,2500,1,32].\n\nCall arguments received by layer \"max_pooling2d_1\" (type MaxPooling2D):\n  â€¢ inputs=tf.Tensor(shape=(None, 2500, 1, 32), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/chris/github/projects/object_detection_keras_unet/01_keras_unet_WORKING_recover.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/github/projects/object_detection_keras_unet/01_keras_unet_WORKING_recover.ipynb#ch0000038?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCreating New Model\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/github/projects/object_detection_keras_unet/01_keras_unet_WORKING_recover.ipynb#ch0000038?line=20'>21</a>\u001b[0m inputs \u001b[39m=\u001b[39m Input((x_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], x_train\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], \u001b[39m3\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chris/github/projects/object_detection_keras_unet/01_keras_unet_WORKING_recover.ipynb#ch0000038?line=21'>22</a>\u001b[0m model \u001b[39m=\u001b[39m make_model(inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/github/projects/object_detection_keras_unet/01_keras_unet_WORKING_recover.ipynb#ch0000038?line=22'>23</a>\u001b[0m                        model_name\u001b[39m=\u001b[39;49mmodel_name)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/github/projects/object_detection_keras_unet/01_keras_unet_WORKING_recover.ipynb#ch0000038?line=24'>25</a>\u001b[0m \u001b[39m# =================== MODEL PARAMS =================== #\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chris/github/projects/object_detection_keras_unet/01_keras_unet_WORKING_recover.ipynb#ch0000038?line=25'>26</a>\u001b[0m validation_plots \u001b[39m=\u001b[39m ValPlotCallback(model, model_batch_size, x_val, y_val)\n",
      "File \u001b[0;32m~/github/projects/object_detection_keras_unet/model.py:25\u001b[0m, in \u001b[0;36mmake_model\u001b[0;34m(inputs, model_name, print_summary)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/chris/github/projects/object_detection_keras_unet/model.py?line=22'>23</a>\u001b[0m c2 \u001b[39m=\u001b[39m Dropout(\u001b[39m0.1\u001b[39m)(c2)\n\u001b[1;32m     <a href='file:///Users/chris/github/projects/object_detection_keras_unet/model.py?line=23'>24</a>\u001b[0m c2 \u001b[39m=\u001b[39m Conv2D(\u001b[39m32\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39melu\u001b[39m\u001b[39m'\u001b[39m, kernel_initializer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhe_normal\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m)(c2)\n\u001b[0;32m---> <a href='file:///Users/chris/github/projects/object_detection_keras_unet/model.py?line=24'>25</a>\u001b[0m p2 \u001b[39m=\u001b[39m MaxPooling2D((\u001b[39m2\u001b[39;49m, \u001b[39m2\u001b[39;49m))(c2)\n\u001b[1;32m     <a href='file:///Users/chris/github/projects/object_detection_keras_unet/model.py?line=26'>27</a>\u001b[0m c3 \u001b[39m=\u001b[39m Conv2D(\u001b[39m64\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39melu\u001b[39m\u001b[39m'\u001b[39m, kernel_initializer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhe_normal\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m)(p2)\n\u001b[1;32m     <a href='file:///Users/chris/github/projects/object_detection_keras_unet/model.py?line=27'>28</a>\u001b[0m c3 \u001b[39m=\u001b[39m Dropout(\u001b[39m0.2\u001b[39m)(c3)\n",
      "File \u001b[0;32m~/github/projects/object_detection_keras_unet/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/chris/github/projects/object_detection_keras_unet/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/chris/github/projects/object_detection_keras_unet/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///Users/chris/github/projects/object_detection_keras_unet/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///Users/chris/github/projects/object_detection_keras_unet/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/chris/github/projects/object_detection_keras_unet/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/github/projects/object_detection_keras_unet/.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1963\u001b[0m, in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/chris/github/projects/object_detection_keras_unet/.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py?line=1959'>1960</a>\u001b[0m   c_op \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[1;32m   <a href='file:///Users/chris/github/projects/object_detection_keras_unet/.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py?line=1960'>1961</a>\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   <a href='file:///Users/chris/github/projects/object_detection_keras_unet/.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py?line=1961'>1962</a>\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/chris/github/projects/object_detection_keras_unet/.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py?line=1962'>1963</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(e\u001b[39m.\u001b[39mmessage)\n\u001b[1;32m   <a href='file:///Users/chris/github/projects/object_detection_keras_unet/.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py?line=1964'>1965</a>\u001b[0m \u001b[39mreturn\u001b[39;00m c_op\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"max_pooling2d_1\" (type MaxPooling2D).\n\nNegative dimension size caused by subtracting 2 from 1 for '{{node max_pooling2d_1/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", explicit_paddings=[], ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 1]](Placeholder)' with input shapes: [?,2500,1,32].\n\nCall arguments received by layer \"max_pooling2d_1\" (type MaxPooling2D):\n  â€¢ inputs=tf.Tensor(shape=(None, 2500, 1, 32), dtype=float32)"
     ]
    }
   ],
   "source": [
    "train = Path(train_image_dir).glob('*')\n",
    "mask = Path(train_mask_dir).glob('*')\n",
    "train = list(train)[:4]\n",
    "mask = list(mask)[:4]\n",
    "\n",
    "train_img_batches = create_batches(train, 2)\n",
    "train_mask_batches = create_batches(mask, 2)\n",
    "\n",
    "for train_stack, mask_stack in zip(\n",
    "    batch_stacker(train_img_batches), batch_stacker(train_mask_batches), strict=True\n",
    "):\n",
    "    # print('train_stack')\n",
    "    # array_properties(train_stack)\n",
    "    # print('mask stack')\n",
    "    # array_properties(mask_stack)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        train_stack, mask_stack, random_state=SEED, test_size=train_val_split_size\n",
    "    )\n",
    "    # print('xtrain')\n",
    "    # array_properties(x_train)\n",
    "    # print('ytrain')\n",
    "    # array_properties(y_train)\n",
    "    # print('xval')\n",
    "    # array_properties(x_val)\n",
    "    # print('yval')\n",
    "    # array_properties(y_val)\n",
    "    # print('-----===================-----------------')\n",
    "    # print()\n",
    "\n",
    "    print('Creating New Model')\n",
    "    inputs = Input((x_train.shape[1], x_train.shape[2], 3))\n",
    "    model = make_model(inputs=inputs, model_name=model_name, print_summary=True)\n",
    "\n",
    "    # =================== MODEL PARAMS =================== #\n",
    "    validation_plots = ValPlotCallback(model, model_batch_size, x_val, y_val)\n",
    "    model_fit_params = dict(\n",
    "        batch_size=model_batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "        verbose=1,\n",
    "        steps_per_epoch=x_train.shape[0] // model_batch_size,\n",
    "        validation_steps=(x_train.shape[0] // model_batch_size) * train_val_split_size,\n",
    "        callbacks=[early_stop, check_point, tensor_board, validation_plots],\n",
    "    )\n",
    "\n",
    "    print('Augmenting Data')\n",
    "\n",
    "    # provide the same seed and keyword arguments\n",
    "    image_datagen = ImageDataGenerator(**datagen_args)\n",
    "    mask_datagen = ImageDataGenerator(**datagen_args)\n",
    "\n",
    "    image_datagen.fit(x_train, augment=True, seed=SEED)\n",
    "    mask_datagen.fit(y_train, augment=True, seed=SEED)\n",
    "\n",
    "    # save_to_dir='../images/augmented/images/',\n",
    "    # save_to_dir='../images/augmented/masks/',\n",
    "    image_generator = image_datagen.flow(x_train, seed=SEED)\n",
    "    mask_generator = mask_datagen.flow(y_train, seed=SEED)\n",
    "    ### NOTE ###\n",
    "    # Can I use the validation setting on the image generators and get rid of the train/test split\n",
    "    # then I can feed my images in as generators to the generators and skip the loop\n",
    "\n",
    "    # combine generators into one which yields image and masks\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "\n",
    "    model.fit(train_generator, **model_fit_params)\n",
    "\n",
    "model.save(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # ================================================================ #\n",
    "    # =========================== TRAINING =========================== #\n",
    "    \n",
    "    # load or train model\n",
    "    if pretrained_model or batch_number > 1:\n",
    "        print('Loading Trained Model')\n",
    "        model = load_model(trained_model)\n",
    "    else:\n",
    "        print('Creating New Model')\n",
    "        inputs = Input((x_train.shape[1], x_train.shape[2], 3))\n",
    "        model = make_model(inputs=inputs,\n",
    "                           model_name=model_name)\n",
    "        \n",
    "    # =================== MODEL PARAMS =================== #\n",
    "    model_fit_params = dict(batch_size=model_batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        verbose=1,\n",
    "                        steps_per_epoch=x_train.shape[0] // model_batch_size,\n",
    "                        validation_steps=(x_train.shape[0] // model_batch_size) * train_val_split_size,\n",
    "                        callbacks=[early_stop, check_point, tensor_board, validation_plots])\n",
    "        \n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        model.fit(x_train, y_train, **model_fit_params)\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "\n",
    "        # provide the same seed and keyword arguments\n",
    "        image_datagen = ImageDataGenerator(**datagen_args)\n",
    "        mask_datagen = ImageDataGenerator(**datagen_args)\n",
    "\n",
    "        image_datagen.fit(x_train, augment=True, seed=seed)\n",
    "        mask_datagen.fit(y_train, augment=True, seed=seed)\n",
    "\n",
    "        # save_to_dir='../images/augmented/images/',\n",
    "        # save_to_dir='../images/augmented/masks/',\n",
    "        image_generator = image_datagen.flow(x_train, seed=seed)\n",
    "        mask_generator = mask_datagen.flow(y_train, seed=seed)\n",
    "        ### NOTE ###\n",
    "        # Can I use the validation setting on the image generators and get rid of the train/test split\n",
    "        # then I can feed my images in as generators to the generators and skip the loop\n",
    "        \n",
    "        # combine generators into one which yields image and masks\n",
    "        train_generator = zip(image_generator, mask_generator)\n",
    "\n",
    "        model.fit(train_generator, **model_fit_params)\n",
    "\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================ #\n",
    "# ========================== VALIDATION ========================== #\n",
    "\n",
    "x_val_pred = model.predict(x_val, verbose=1, batch_size=model_batch_size)\n",
    "\n",
    "model.evaluate(x=x_val, y=y_val, batch_size=model_batch_size)\n",
    "\n",
    "# simple threshold to change to 1/0, mask\n",
    "x_val_pred_mask = (x_val_pred > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(original=x_val, predicted=x_val_pred,\n",
    "                 predicted_mask=x_val_pred_mask, ground_truth=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('../data/testing_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================ #\n",
    "# ========================== PREDICTION ========================== #\n",
    "\n",
    "\n",
    "x_test = [np.array(\n",
    "    image_splitter(\n",
    "        cv2.imread(test_image_dir + img_name).astype(np.uint8),\n",
    "        num_col_splits=split_cols,\n",
    "        num_row_splits=split_rows,\n",
    "        resize=resize,\n",
    "        resize_height=image_resize_height,\n",
    "        resize_width=image_resize_width\n",
    "    )\n",
    ") for img_name in tqdm(test_filenames[:20])]\n",
    "\n",
    "x_test = (np.vstack(x_test)/255).astype(np.float32)\n",
    "\n",
    "shape_and_mem(x_test)\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "test_generator = test_datagen.flow(x_test, seed=seed)\n",
    "\n",
    "y_pred = model.predict(test_generator, verbose=1)\n",
    "\n",
    "shape_and_mem(y_pred)\n",
    "\n",
    "y_pred_mask = (y_pred > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_predictions(original=x_test, predicted=y_pred,\n",
    "                 predicted_mask=y_pred_mask)\n",
    "\n",
    "\n",
    "# https://www.jeremyjordan.me/evaluating-image-segmentation-models/\n",
    "\n",
    "# result = cv2.bitwise_and(test_split[0], test_split[0], mask=prediction[0])\n",
    "\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2\n",
    "\n",
    "# https://www.jeremyjordan.me/evaluating-image-segmentation-models/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cca29d56564b7cec1d60ca6d75a12513a3462fa36dc05318b6fa4375581983a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
