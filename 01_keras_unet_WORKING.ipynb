{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "https://www.kaggle.com/christofhenkel/keras-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#########################################################################\n",
    "# CURRENTLY WORKING WITH TENSORFLOW\n",
    "# \n",
    "# PRIORITIES:\n",
    "# 1. refactor code to better groupings\n",
    "# 2. where do the functions go\n",
    "# 3. grab validation data\n",
    "# 4. test testing data generator\n",
    "\n",
    "#########################################################################\n",
    "\n",
    "\n",
    "# ====================================== #\n",
    "### CURRENT WORKING IMAGE INPUT PARAMS ###\n",
    "\n",
    "# IMAGES_PER_BATCH\n",
    "# original images loaded and vstacked\n",
    "# 16 images = 23 GB\n",
    "# 8 images is faster\n",
    "\n",
    "# MODEL_BATCH_SIZE\n",
    "# how many of the splitter output arrays to process at one time\n",
    "# 128 = ERROR\n",
    "# 96 is okay\n",
    "\n",
    "# SPLITS\n",
    "# row X col = total images into array\n",
    "# more splits = larger batch size\n",
    "\n",
    "### GOOD ###\n",
    "images_per_batch = 16\n",
    "split_rows = 20\n",
    "split_cols = 20\n",
    "model_batch_size = 96\n",
    "augmentation = False\n",
    "\n",
    "### NOT GOOD ###\n",
    "images_per_batch = 16\n",
    "split_rows = 20\n",
    "split_cols = 20\n",
    "model_batch_size = 96\n",
    "augmentation = True\n",
    "# with augmentation 28 GB used memory, going into swap\n",
    "\n",
    "### NOT GOOD ###\n",
    "images_per_batch = 16\n",
    "split_rows = 24\n",
    "split_cols = 24\n",
    "model_batch_size = 128\n",
    "augmentation = True\n",
    "# batch size too large to fit in GPU\n",
    "\n",
    "\n",
    "# ====================================== #\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ======================================== FOR RUNNING ON THE MACBOOK PRO ====================\n",
    "\n",
    "### TODO ###\n",
    "# Check for MacOS environment to enable this automatically\n",
    "\n",
    "# ## DO THIS BEFORE IMPORTING KERAS OR TENSOR TO USE PLAIDML\n",
    "# import plaidml.keras\n",
    "# plaidml.keras.install_backend()\n",
    "\n",
    "# # Help MacOS be able to use Keras\n",
    "# import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "\n",
    "# # Gets rid of the processor warning.\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from callbacks import ValPlotCallback\n",
    "from model import make_model\n",
    "from helpers import array_shape_and_mem_usage\n",
    "from keras.layers import Input\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from tqdm.keras import TqdmCallback\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# import skimage.io\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Params\n",
    "\n",
    "# val split from training set\n",
    "train_val_split_size = .1\n",
    "\n",
    "seed = 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Parameters\n",
    "\n",
    "train_image_dir = '../images/network_training/build/0/'\n",
    "train_mask_dir = '../images/network_training/mask/0/'\n",
    "test_image_dir = '../images/network_training/test/0/'\n",
    "data_dir = '../data/'\n",
    "\n",
    "train_images = glob.glob(train_image_dir + '*')\n",
    "train_filenames = [os.path.basename(x) for x in train_images]\n",
    "\n",
    "test_images = glob.glob(test_image_dir + '*')\n",
    "test_filenames = [os.path.basename(x) for x in test_images]\n",
    "\n",
    "images_per_batch = 8\n",
    "# decrease this number if running out of memory\n",
    "\n",
    "\n",
    "# ======Image Splitter Params ====== #\n",
    "\n",
    "split_rows = 20\n",
    "split_cols = 20\n",
    "resize = True\n",
    "image_resize_width = 4800\n",
    "image_resize_height = 4800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create batch sets of 'images_per_batch' size\n",
    "training_sets = [train_filenames[i:i + images_per_batch]\n",
    "                 for i in range(0, len(train_filenames), images_per_batch)]\n",
    "training_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images in batch as a list of arrays\n",
    "for batch_number, train_set in enumerate(tqdm(training_sets), start=1):\n",
    "    print(f'BATCH NUMBER {batch_number} of {len(training_sets)}')\n",
    "    xlist = []\n",
    "    ylist = []\n",
    "    for name in tqdm(train_set):\n",
    "        \n",
    "        xlist.append(load_image_as_array(name, train_image_dir, resize=True))\n",
    "        ylist.append(load_image_as_array(name, train_mask_dir, resize=True, gray=True))\n",
    "\n",
    "    print('loadedimages')\n",
    "    print('xlist')\n",
    "    print(len(xlist))\n",
    "    print(xlist[0].shape)\n",
    "    print('ylist')\n",
    "    print(len(ylist))\n",
    "    print(ylist[0].shape)\n",
    "\n",
    "    x = (np.vstack(xlist)/255).astype(np.float32)\n",
    "    y = (np.vstack(ylist)/255).astype(np.float32)\n",
    "    y = np.expand_dims(y, axis=3)  # grayscale\n",
    "\n",
    "    print('stacked images')\n",
    "    print('x and y')\n",
    "    print(array_shape_and_mem_usage(x))\n",
    "    print(array_shape_and_mem_usage(y))\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y,\n",
    "                                                      random_state=77,\n",
    "                                                      test_size=train_val_split_size)\n",
    "    print('xtrain, ytrain, xval, yval')\n",
    "    print(array_shape_and_mem_usage(x_train))\n",
    "    print(array_shape_and_mem_usage(y_train))\n",
    "    print(array_shape_and_mem_usage(x_val))\n",
    "    print(array_shape_and_mem_usage(y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ==================================== MODEL PARAMS ==================================== #\n",
    "epochs = 10\n",
    "model_batch_size = 64\n",
    "model_name = 'datagenmodel'\n",
    "model_path = os.path.join(data_dir, model_name + '.h5')\n",
    "pretrained_model = True\n",
    "\n",
    "pretrained_model_path = model_path\n",
    "\n",
    "print_model_summary_on_compile = False\n",
    "\n",
    "plot_epoch_val_images = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== Callbacks ==================== #\n",
    "\n",
    "\n",
    "\n",
    "validation_plots = ValPlotCallback()\n",
    "early_stop = EarlyStopping(patience=5, verbose=1)\n",
    "check_point = ModelCheckpoint(os.path.join(data_dir, model_name + '.hdf5'),\n",
    "                              save_best_only=True,verbose=1)\n",
    "tensor_board = TensorBoard(log_dir='../logs/tensorboard/', histogram_freq=1, \n",
    "                            write_graph=True, write_grads=False, \n",
    "                            write_images=True, embeddings_freq=1, update_freq='epoch')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================== Data Augmentation ==================== #                        \n",
    "data_augmentation = True\n",
    "datagen_args = dict(featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                             samplewise_center=False,  # set each sample mean to 0\n",
    "                             featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                             samplewise_std_normalization=False,  # divide each input by its std\n",
    "                             zca_whitening=False,  # apply ZCA whitening\n",
    "                             zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "                             # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                             rotation_range=60,\n",
    "                             # randomly shift images horizontally (fraction of total width)\n",
    "                             width_shift_range=0.2,\n",
    "                             # randomly shift images vertically (fraction of total height)\n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.,  # set range for random shear\n",
    "                             zoom_range=0.,  # set range for random zoom\n",
    "                             channel_shift_range=0.,  # set range for random channel shifts\n",
    "                             # set mode for filling points outside the input boundaries\n",
    "                             fill_mode='nearest',\n",
    "                             cval=0.,  # value used for fill_mode = \"constant\"\n",
    "                             horizontal_flip=True,  # randomly flip images\n",
    "                             vertical_flip=True,  # randomly flip images\n",
    "                             # set rescaling factor (applied before any other transformation)\n",
    "                             rescale=None,\n",
    "                             # set function that will be applied on each input\n",
    "                             preprocessing_function=None,\n",
    "                             # image data format, either \"channels_first\" or \"channels_last\"\n",
    "                             data_format='channels_last',\n",
    "                             # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "                             validation_split=0.0)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # ================================================================ #\n",
    "    # =========================== TRAINING =========================== #\n",
    "    \n",
    "    # load or train model\n",
    "    if pretrained_model or batch_number > 1:\n",
    "        print('Loading Trained Model')\n",
    "        model = load_model(trained_model)\n",
    "    else:\n",
    "        print('Creating New Model')\n",
    "        inputs = Input((x_train.shape[1], x_train.shape[2], 3))\n",
    "        model = make_model(inputs=inputs,\n",
    "                           model_name=model_name)\n",
    "        \n",
    "    # =================== MODEL PARAMS =================== #\n",
    "    model_fit_params = dict(batch_size=model_batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        verbose=1,\n",
    "                        steps_per_epoch=x_train.shape[0] // model_batch_size,\n",
    "                        validation_steps=(x_train.shape[0] // model_batch_size) * train_val_split_size,\n",
    "                        callbacks=[early_stop, check_point, tensor_board, validation_plots])\n",
    "        \n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        model.fit(x_train, y_train, **model_fit_params)\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "\n",
    "        # provide the same seed and keyword arguments\n",
    "        image_datagen = ImageDataGenerator(**datagen_args)\n",
    "        mask_datagen = ImageDataGenerator(**datagen_args)\n",
    "\n",
    "        image_datagen.fit(x_train, augment=True, seed=seed)\n",
    "        mask_datagen.fit(y_train, augment=True, seed=seed)\n",
    "\n",
    "        # save_to_dir='../images/augmented/images/',\n",
    "        # save_to_dir='../images/augmented/masks/',\n",
    "        image_generator = image_datagen.flow(x_train, seed=seed)\n",
    "        mask_generator = mask_datagen.flow(y_train, seed=seed)\n",
    "        ### NOTE ###\n",
    "        # Can I use the validation setting on the image generators and get rid of the train/test split\n",
    "        # then I can feed my images in as generators to the generators and skip the loop\n",
    "        \n",
    "        # combine generators into one which yields image and masks\n",
    "        train_generator = zip(image_generator, mask_generator)\n",
    "\n",
    "        model.fit(train_generator, **model_fit_params)\n",
    "\n",
    "    model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================ #\n",
    "# ========================== VALIDATION ========================== #\n",
    "\n",
    "x_val_pred = model.predict(x_val, verbose=1, batch_size=model_batch_size)\n",
    "\n",
    "model.evaluate(x=x_val, y=y_val, batch_size=model_batch_size)\n",
    "\n",
    "# simple threshold to change to 1/0, mask\n",
    "x_val_pred_mask = (x_val_pred > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(original=x_val, predicted=x_val_pred,\n",
    "                 predicted_mask=x_val_pred_mask, ground_truth=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('../data/testing_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================ #\n",
    "# ========================== PREDICTION ========================== #\n",
    "\n",
    "\n",
    "x_test = [np.array(\n",
    "    image_splitter(\n",
    "        cv2.imread(test_image_dir + img_name).astype(np.uint8),\n",
    "        num_col_splits=split_cols,\n",
    "        num_row_splits=split_rows,\n",
    "        resize=resize,\n",
    "        resize_height=image_resize_height,\n",
    "        resize_width=image_resize_width\n",
    "    )\n",
    ") for img_name in tqdm(test_filenames[:20])]\n",
    "\n",
    "x_test = (np.vstack(x_test)/255).astype(np.float32)\n",
    "\n",
    "shape_and_mem(x_test)\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "test_generator = test_datagen.flow(x_test, seed=seed)\n",
    "\n",
    "y_pred = model.predict(test_generator, verbose=1)\n",
    "\n",
    "shape_and_mem(y_pred)\n",
    "\n",
    "y_pred_mask = (y_pred > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_predictions(original=x_test, predicted=y_pred,\n",
    "                 predicted_mask=y_pred_mask)\n",
    "\n",
    "\n",
    "# https://www.jeremyjordan.me/evaluating-image-segmentation-models/\n",
    "\n",
    "# result = cv2.bitwise_and(test_split[0], test_split[0], mask=prediction[0])\n",
    "\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2\n",
    "\n",
    "# https://www.jeremyjordan.me/evaluating-image-segmentation-models/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cca29d56564b7cec1d60ca6d75a12513a3462fa36dc05318b6fa4375581983a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
